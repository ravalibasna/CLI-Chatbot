{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_message:1+1?\n",
      "[{'role': 'user', 'content': [{'text': '1+1?'}]}]\n",
      "assistant:2\n",
      "ERROR: Can't invoke 'anthropic.claude-3-haiku-20240307-v1:0'. Reason: add_assistant_message() takes 1 positional argument but 2 were given\n",
      "user_message:add 3\n",
      "[{'role': 'user', 'content': [{'text': '1+1?'}]}, {'role': 'user', 'content': [{'text': 'add 3'}]}]\n",
      "assistant:Okay, let's solve this step-by-step:\n",
      "\n",
      "1 + 1 = 2\n",
      "Then, we add 3 to the result:\n",
      "2 + 3 = 5\n",
      "\n",
      "So, the final answer is 5.\n",
      "ERROR: Can't invoke 'anthropic.claude-3-haiku-20240307-v1:0'. Reason: add_assistant_message() takes 1 positional argument but 2 were given\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region you want to use.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# Set the model ID, e.g., Claude 3 Haiku.\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "conversation = []\n",
    "def add_user_message(message):\n",
    "    conversation.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": message}],\n",
    "        }\n",
    "    )\n",
    "\n",
    "def add_assistant_message(message):\n",
    "    conversation.append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"text\": message}],\n",
    "        }\n",
    "    )\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"You: \")\n",
    "    add_user_message(user_message)\n",
    "    try:\n",
    "        print(\"user_message:\" + user_message)\n",
    "        print(conversation)\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        response = client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=conversation,\n",
    "            inferenceConfig={\"maxTokens\": 512, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "        )\n",
    "\n",
    "        # Extract and print the response text.\n",
    "        response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        print(\"assistant:\" + response_text)\n",
    "        add_assistant_message(response_text)\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai-env)",
   "language": "python",
   "name": "fastai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
